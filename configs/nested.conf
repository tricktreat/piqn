train_path = data/datasets/ace05/ace05_train_context.json
valid_path = data/datasets/ace05/ace05_dev_context.json
save_path = data/checkpoints/ace05
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 8
epochs = 50
lr = 2e-05
lr_warmup = 0.1
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 45
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/ace05/ace05_types.json
tokenizer_path = bert-large-cased
lowercase = False
sampling_processes = 4
label = ace05_train
log_path = data/checkpoints/ace05
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 0
model_path = bert-large-cased
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = True
use_token_level_encoder = True
num_token_entity_encoderlayer = 6
use_entity_attention = False
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = True
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None


train_path = data/datasets/ace04/ace04_train_context.json
valid_path = data/datasets/ace04/ace04_dev_context.json
save_path = data/checkpoints/ace04
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 8
epochs = 50
lr = 2e-05
lr_warmup = 0.1
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 45
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/ace04/ace04_types.json
tokenizer_path = bert-large-cased
lowercase = False
sampling_processes = 4
label = ace04_train
log_path = data/checkpoints/ace04
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 2
model_path = bert-large-cased
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = False
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = True
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = False
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None

train_path = data/datasets/genia/genia_train_dev_context.json
valid_path = data/datasets/genia/genia_test_context.json
save_path = data/checkpoints/genia
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 4
epochs = 30
lr = 2e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 30
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/genia/genia_types.json
tokenizer_path = dmis-lab/biobert-large-cased-v1.1
lowercase = False
sampling_processes = 4
label = genia_train
log_path = data/checkpoints/genia
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 3
model_path = dmis-lab/biobert-large-cased-v1.1
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../biovec/PubMed-shuffle-win-30.txt
share_query_pos = True
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = True
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = False
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None

train_path = data/datasets/kbp17/kbp17_train_context.json
valid_path = data/datasets/kbp17/kbp17_dev_context.json
save_path = data/checkpoints/kbp17
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 4
epochs = 30
lr = 3e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 1.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 30
split_epoch = 5
copy_weight = True
local_rank = -1
world_size = -1
types_path = data/datasets/kbp17/kbp17_types.json
tokenizer_path = bert-base-cased
lowercase = False
sampling_processes = 4
label = kbp17_train
log_path = data/checkpoints/kbp17
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 3
model_path = bert-base-cased
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = False
use_token_level_encoder = True
num_token_entity_encoderlayer = 3
use_entity_attention = True
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = False
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None

train_path = data/datasets/nne/nne_train_context.json
valid_path = data/datasets/nne/nne_dev_context.json
save_path = data/checkpoints/nne
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 4
epochs = 50
lr = 2e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 30
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/nne/nne_types.json
tokenizer_path = bert-large-cased
lowercase = False
sampling_processes = 4
label = nne_train
log_path = data/checkpoints/nne
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 3
model_path = bert-large-cased
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = True
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = False
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = False
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None


